{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\admin\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 2.084 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "##使用python分词工具包jieba对微博文本进行分词\n",
    "##分词原则：只保留名词和英文，过滤停止词和无意义词，过滤分词后单词个数小于3或大于70的微博，过滤低频词（出现次数小于3）\n",
    "import jieba\n",
    "import jieba.posseg as pseg\n",
    "jieba.load_userdict('userdict.txt')\n",
    "filter_list = ['网页','链接','正文']##过滤单词列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##导入uid list\n",
    "def load_uid():\n",
    "    uid_list = []\n",
    "    with open('../../data/uid.txt','r',encoding = 'utf-8') as f1:\n",
    "        while True:\n",
    "            line = f1.readline()\n",
    "            if not line:\n",
    "                break\n",
    "            uid_list.append(line.strip())\n",
    "        f1.close()\n",
    "    return uid_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##分词，只保留名词和英文，过滤停止词和filter\n",
    "def split_text(text):\n",
    "        result = pseg.cut(text) ##jieba分词\n",
    "        word_list = []\n",
    "        for w in result:\n",
    "            #w.flag为词性\n",
    "            flag = str(w.flag) \n",
    "            #w.word为单词\n",
    "            word = str(w.word)\n",
    "            word = word.lower()\n",
    "            #print(w.word + '/' + w.flag)\n",
    "            if word in filter_list: ##过滤单词列表\n",
    "                continue\n",
    "            if word in stopwords: ##过滤停止词\n",
    "                continue\n",
    "            if flag.startswith('n') or flag == 'eng': ##只保留名词和英文\n",
    "                word_list.append(word)\n",
    "        if len(word_list) < 3 or len(word_list) > 70: ##单词数小于3或大于70的微博过滤\n",
    "            return []\n",
    "        return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##过滤低频词：出现次数小于三次\n",
    "def low_fre_filter(result):\n",
    "    fre = {}\n",
    "    for item in result.values():\n",
    "        for word in item:\n",
    "            if word not in fre:\n",
    "                fre[word] = 0\n",
    "            fre[word] += 1\n",
    "    tmp = {}\n",
    "    avg = 0\n",
    "    for id in result.keys():\n",
    "        lis = result[id]\n",
    "        tmp[id] = []\n",
    "        for word in lis:\n",
    "            if fre[word] > 2:\n",
    "                tmp[id].append(word)\n",
    "        avg += len(tmp[id])\n",
    "    #print(avg/len(tmp))\n",
    "    result = tmp\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##读入停止词表\n",
    "def load_stopwords():\n",
    "    filename =  '../../data/text/stopwords.txt'\n",
    "    stopwords = []\n",
    "    with open(filename,'r',encoding='utf-8') as f:\n",
    "        while True:\n",
    "            line = f.readline()\n",
    "            if not line:\n",
    "                break\n",
    "            line = line.strip()\n",
    "            stopwords.append(line)\n",
    "        f.close()\n",
    "    return stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    uid_list = load_uid()\n",
    "    for uid in uid_list:\n",
    "        filename = '../../data/text/before_split/'+uid+'.txt'\n",
    "        ## 读入微博文本\n",
    "        with open(filename,'r',encoding=\"utf-8\") as f:\n",
    "            dic = f.readline().strip()\n",
    "            dic = eval(dic)\n",
    "            f.close()\n",
    "        stopwords=load_stopwords() ##导入停止词\n",
    "        result = {}\n",
    "        for item in dic.keys():\n",
    "            text = split_text(dic[item]) ##分词，返回分词后的list\n",
    "            if len(text) == 0:\n",
    "                continue\n",
    "            result[item] = text\n",
    "        retuslt = low_fre_filter(result) ##过滤低频词\n",
    "        ##分词结果写入after_filter\n",
    "        output = '../../data/text/after_filter/'+uid+'.dat'\n",
    "        with open (output,'w',encoding='utf-8') as f2:\n",
    "            for id in result.keys():\n",
    "                f2.write(id+':')\n",
    "                for item in result[id]:\n",
    "                    f2.write(item+' ')\n",
    "                f2.write('\\n')\n",
    "            f2.close()        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
